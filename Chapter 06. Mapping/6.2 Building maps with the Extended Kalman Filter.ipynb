{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 Building maps with the Extended Kalman Filter - The Nirvana shopping mall\n",
    "\n",
    "The managers behind the **<span style=\"color:seagreen\">Nirvana shopping mall</span>**, in the scope of their *experiencing the future* plan, have contacted **<span style=\"color:seagreen\">UMA-MR</span>** (our brand-new mobile robotics company at UMA) looking for mobile robots able to guide their visitors between different points of interest in their facilities, like information points, the entrance to relevant (paying well) shops, rescue points, etc. \n",
    "\n",
    "<br /><br />\n",
    "<center>\n",
    "<img src=\"./images/nirvana-logo.png\" width=\"600\">\n",
    "</center>\n",
    "<br />\n",
    "\n",
    "These managers have placed identifying marks close to the points of interest they want to consider, but do not know their exact location in the mall. **Our mission as engineers at *UMA-MR* is to build a map of the mall containing such points, so the robot can operate within it.** We are going to use for that an **Extended Kalman Filter (EKF)**.\n",
    "\n",
    "Fortunately, our company has developed a system able to provide the exact location of our robots at each time instant, which are also equipped with a range-and-bearing nosiy sensor able to detect the identifying marks and take measurements to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.1 Formalizing the problem\n",
    "\n",
    "Since we are going to build a map of $N$ landmarks, the position of those landmarks in the map $m$ are the random variables to be estimated. In this way, the state vector in the mapping case is defined as:\n",
    "\n",
    "$$m = [m_1, m_2, \\cdots, m_N] = \n",
    "[x_1, y_1, \\underbrace{x_2, y_2}_{\\text{Position of}\\\\\\text{landmark 2}}, \\dots,x_N, y_N]^T, \\ \\ \\ len(m) = 2N$$\n",
    "\n",
    "In other words, we pursuit the estimation of the probability distribution:\n",
    "\n",
    "$$p(m | z_{1:t}, x_{1:t})$$\n",
    "\n",
    "being $z_{1:t}$ the sensor measurements taken until time instant $t$, and $x_{1:t}$ the robot poses from which those measurements were acquired. Recall that a sensor measurement is related to the pose $x$ and the map $m$ by means of the **observation function**:\n",
    "\n",
    "$$z = h(x,m) + e \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, e \\sim N(0,Q)$$\n",
    "\n",
    "Two assumptions are made when building maps using EFK.\n",
    "\n",
    "#### Assumption 1: each landmark position is estimated independently\n",
    "\n",
    "For simplifying the problem, it is usual to assume that the estimation of the position of the landmarks is independent one to another, only depending each one on its observations, so:\n",
    "\n",
    "$$p(m_i|z_{1:t}, x_{1:t}) = p(m_i |z^i_{1:t}, x_{1:t})$$\n",
    "\n",
    "which results in the following simplification:\n",
    "\n",
    "$$p(m | z_{1:t}, x_{1:t})= p(m_1, m_2, \\cdots, m_N | z_{1:t}, x_{1:t}) = \\prod_{k=1}^N p(m_i | z^i_{1:t}, x_{1:t})$$\n",
    "\n",
    "As a consequence of this, the estimation of, for example, 3 landmarks simultaneously is the same as using three concurrent and independent EKFs for estimating them. \n",
    "\n",
    "#### Assumption 2: the map is static\n",
    "\n",
    "Unlike the localization case, where the robot pose changed over time, in this case it is assumed that the map is static, that is, the landmarks are still. This means that in the state transition model $m_k=m_{k-1}$, that is:\n",
    "\n",
    "$$m_t = A_t m_{t-1} + B_t u_t + \\epsilon_t \\ \\ \\ \\ (A=I, u=0, \\epsilon=0)$$\n",
    "\n",
    "So good news here!, there is no need for a prediction step in the EKF, we only have to model the correction (update) one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.2 Developing the EKF filter for mapping the mall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from scipy import linalg\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils.tcomp import tcomp\n",
    "from utils.AngleWrap import AngleWrap\n",
    "from utils.DrawRobot import DrawRobot\n",
    "from utils.PlotEllipse import PlotEllipse\n",
    "from utils.Jacobians import J2\n",
    "from utils.unit6.MapCanvas import MapCanvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The provided robot\n",
    "\n",
    "Take a look at the following ``EFKMappingRobot()`` class, which is already provided by your colleagues at **<span style=\"color:seagreen\">UMA-MR</span>**, modeling **a robot equipped with a range and bearing sensor**, which is able to keep the following information:\n",
    "- ``true_pose``: The exact pose of the robot in the environment, which is perfectly known.\n",
    "- ``Q``: Uncertainty of the range and bearing sensor, which has the form:\n",
    "$\\Sigma_{r\\theta} = \\begin{bmatrix} \\sigma^2_r & 0 \\\\ 0 & \\sigma^2_\\theta  \\end{bmatrix}$\n",
    "- ``xEst``: Vector with the estimated state, in this case the position of the $M$ observed landmarks: $[x_1, y_1, x_2, y_2, \\dots,x_M, y_M]^T$. Its size changes over time with the detection of previously unobserved landmarks.\n",
    "- ``PEst``: uncertainty associated with those predictions, with size ($M\\times2$,$M\\times2$). Its size also changes over time.\n",
    "- ``MappedLandmarks``: A vector with length equal to the number of landmarks in the map ($N$), which elements can take the following values:\n",
    "  - ``-1`` if the landmark with that index has not been seen yet.\n",
    "  - ``idx_in_xEst``: an odd number indicating the position of that landmark in ``xEst``.\n",
    "  For example, if during the robot operation in a map with 5 landmarks, it first detect the landmark with id 2, and later the one with id 4, the content of this vector would be ``MappedLandmarks=[-1,-1,1,-1,3]``.\n",
    "\n",
    "and to perform the following actions:\n",
    "\n",
    "- ``step()``: Performs a motion command, without noise. \n",
    "- ``observe()``: Returns a range and bearing measurement (in polars) to a given landmark in the map.\n",
    "- ``get_random_observation()``: Returns a range and bearing measurement (in polars) to a random landmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EFKMappingRobot():\n",
    "    def __init__(self, true_pose, sigma_r, sigma_theta, n_features):\n",
    "        # Robot description\n",
    "        self.true_pose = true_pose\n",
    "        self.Q = np.diag([sigma_r, sigma_theta])**2\n",
    "        \n",
    "        # Map -- Initially empty\n",
    "        self.xEst = np.empty((0, 0))\n",
    "        self.PEst = np.empty((0, 0))\n",
    "        self.QEst = 1.0*self.Q\n",
    "        \n",
    "        self.MappedLandmarks = -1*np.ones((n_features,1), int)\n",
    "        \n",
    "    def step(self, u):\n",
    "        self.true_pose = tcomp(self.true_pose, u)\n",
    "        \n",
    "    def observe(self, idx, world, noisy=True):\n",
    "        \"\"\" Generate a observation of a feature in our world\n",
    "        \n",
    "            Args:\n",
    "                world: Complete map of all landmarks in the world\n",
    "                idx: Landmark to observe (index in world matrix)\n",
    "                noisy: Add noise to z, prorportional to self.Q\n",
    "                \n",
    "            Returns:\n",
    "                z: One range and bearing observation\n",
    "        \"\"\"\n",
    "        \n",
    "        z = np.empty((2, 1))\n",
    "        delta = world[:, [idx]] - self.true_pose[0:2, :]\n",
    "        \n",
    "        # Range\n",
    "        z[0, :] = np.sqrt(np.sum(delta**2))\n",
    "        # Bearing\n",
    "        z[1, :] = np.arctan2(delta[1, 0],delta[0, 0]) - self.true_pose[2, 0]\n",
    "        z[1, :] = AngleWrap(z[1, :])\n",
    "        \n",
    "        if noisy:\n",
    "            z = z + np.sqrt(self.Q)@random.randn(2,1)\n",
    "            \n",
    "        return z\n",
    "    \n",
    "    def get_random_observation(self, world, noisy=True):\n",
    "        iLandmarks = world.shape[1]\n",
    "        iLandmark = random.randint(iLandmarks)\n",
    "        z = self.observe(iLandmark, world, noisy)\n",
    "        return z, [iLandmark]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The prediction step\n",
    "\n",
    "As commented, the map is considered static, so the prediction step is reduced to consider as the predicted landmarks' positions the ones estimated in the previous step. The same holds for the predicted uncertainty, so this steps results in something like this:\n",
    "\n",
    "$$\n",
    "  \\begin{aligned}\n",
    "       \\verb!def !& \\verb!ExtendedKalmanFilter!(m_{t-1},\\Sigma_{t-1}, z_t): \\\\\n",
    "      & \\textbf{Prediction.} \\\\\n",
    "      & \\bar m_t = m_{t-1} &\\text{(1. Map prediction)}\\\\\n",
    "      & \\bar\\Sigma_t = \\Sigma_{t-1} &\\text{(2. Uncertainty of prediction)}\\\\      \n",
    "  \\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 1: Implementing the naive prediction step</i></b></span>** \n",
    "\n",
    "**You are tasked to** implement the previous behavior in the following function, which performs the prediction step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_step(robot: EFKMappingRobot): \n",
    "    \"\"\" Performs the prediction step of the EKF algorithm for mapping\n",
    "            robot: Robot base (contains state map: xEst, PEst)\n",
    "        \n",
    "        Returns: Nothing. But it modifies the state in robot\n",
    "            xPred: Predicted position of the landmarks\n",
    "            PPred: Predicted uncertainty of the landmarks positions\n",
    "    \"\"\"    \n",
    "    \n",
    "    # We assume that the map is static \n",
    "    xPred = robot.xEst\n",
    "    # robot.PEst: Uncertainty of the landmark positions at t-1\n",
    "    PPred = robot.PEst\n",
    "    \n",
    "    return xPred, PPred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can **test your function** with the next code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xPred:\n",
      "[[0.5]\n",
      " [0.7]]\n",
      "PPred:\n",
      "[[1.32 0.  ]\n",
      " [0.   0.8 ]]\n"
     ]
    }
   ],
   "source": [
    "# TRY IT!\n",
    "xVehicleTrue = np.vstack([0.5, 0.7, 0]) # We know the exact robot pose at any moment!\n",
    "robot = EFKMappingRobot(xVehicleTrue, 1, 0.8, 1)\n",
    "robot.xEst = np.vstack([.5, .7])\n",
    "robot.PEst = np.diag([1.32, 0.8])\n",
    "[xPred,PPred] = prediction_step(robot)\n",
    "print('xPred:\\n' + str(xPred))\n",
    "print('PPred:\\n' + str(PPred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Expected output:</span>\n",
    "\n",
    "```\n",
    "xPred:\n",
    "[[0.5]\n",
    " [0.7]]\n",
    "PPred:\n",
    "[[1.32 0.  ]\n",
    " [0.   0.8 ]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observing a landmark for first time\n",
    "\n",
    "When the sensor onboard the robot detects a landmark for the first time, there is no need to do the EKF update step (indeed, since there is not previously knowledge about the landmark, there is nothing to update). Instead, we have to properly modify 1) the vector of estimated landmark positions, and 2) their associated uncertainties, to accommodate this new information:\n",
    "\n",
    "\n",
    "1. **Modifying the state vector**: Insert the position of the new observed landmark $[x_{M+1},y_{M+1}]$, using the sensor measurement $z_k=[r_k,\\theta_k]$, at the end of the vector containing the estimated positions ``xEst``, so: $\\\\[10pt]$\n",
    "$$xEst=[x_1,y_y, \\cdots, x_M, y_M, x_{M+1}, y_{M+1}]\\\\[5pt]$$ \n",
    "Since the measurment is provided in polar coordinates in the robot local frame, we have to convert them first to cartensians and then to the world frame using the robot pose $[x_v,y_v]'$. The function in charge of doing so can be defined as:\n",
    "\n",
    "$$ \n",
    "f(x_v,z_k)=\\begin{bmatrix} x_{M+1} \\\\ y_{M+1} \\end{bmatrix} =\n",
    "\\begin{bmatrix} x_v \\\\ y_v \\end{bmatrix} + \n",
    "r_k\\begin{bmatrix} cos \\alpha_k \\\\ sin \\alpha_k \\end{bmatrix}\n",
    ", \\ \\ \\alpha_k = \\theta_k + \\theta_v\n",
    "$$\n",
    "\n",
    "2. **Extending the covariance matrix**. In order to acomodate the uncertainty regarding the position of the new landmark, we have to extend the covariance matrix in the following way: $\\\\[10pt]$\n",
    "$$\n",
    "PEst=\\begin{bmatrix}\n",
    "    [\\Sigma^1_{xy}]_{2 \\times 2} & \\cdots & 0_{2 \\times 2} \\\\\n",
    "    \\vdots & \\ddots & \\vdots \\\\\n",
    "    0_{2 \\times 2} & \\cdots & [\\Sigma^{M+1}_{xy}]_{2 \\times 2}\n",
    "  \\end{bmatrix}_{2n \\times 2n}\n",
    "  $$  \n",
    "Notice that the covariance $\\Sigma^{M+1}_{xy}$ stands for the uncertainty in the measurement expressed in the world cartesian coordinates, retrieved by: $\\\\[10pt]$\n",
    "$$\\Sigma^{M+1}_{xy} = J \\Sigma^{M+1}_{r\\theta} J^T \\\\[5pt]$$ \n",
    "being $\\Sigma^{M+1}_{r\\theta}$ the uncertainty characterizing the sensor measurements (``QEst`` in our code), and $J$ (``jGz`` in our code) the jacobian of the function $f(x_v,z_k)$ that expresses the measurement in global coordinates, which is: $\\\\[10pt]$\n",
    "$$\n",
    "J =  \\begin{bmatrix} \\partial x / \\partial r  &  \\partial x / \\partial \\theta \\\\ \\partial y / \\partial r & \\partial y / \\partial \\theta \\end{bmatrix} =\n",
    "\\begin{bmatrix} cos \\alpha & -r sin \\alpha \\\\ sin \\alpha & r cos \\alpha \\end{bmatrix} \\\\[10pt]\n",
    "$$\n",
    "Notice that this jacobian is the result of concatenating both, the jacobian of the composition of a pose and a landmark, and the jacobian of the function transforming polar into cartesian coordinates. It is quite similar to the one just transforming polar into cartesian, but in this case the angle  𝛼  is the sum of the robot orientation and the measured angle (bearing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 2: Incorporating a landmark detected for first time</i></b></span>** \n",
    "\n",
    "**Your work here is to:**\n",
    "- Complete the ``get_new_landmark_jacobians()`` to compute the jacobian $J$.\n",
    "- Complete the ``incorporate_new_landmark()`` method to modify the state vector ``xEst`` and the convariance matrix ``PEst`` as explained above. We will make use of the [`linalg.block_diag()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.block_diag.html) function at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_landmark_jacobians(Xv, z):\n",
    "    \"\"\" Calculate the jacobian for transforming an observation to the world frame\n",
    "    \n",
    "        Args:\n",
    "            Xv: True pose of our robot\n",
    "            z: Observation of a landmark. In polar coordinates from the p.o.v. of our robot.\n",
    "        \n",
    "        Returns:\n",
    "            2x2 matrix containing the corresponding jacobian.\n",
    "    \"\"\"\n",
    "    # z is a column vector\n",
    "    r, a = z[0,0], z[1,0]+Xv[2,0]\n",
    "    c, s = np.cos(a), np.sin(a)\n",
    "    jGz = np.array([\n",
    "        [c, -r*s],\n",
    "        [s, r*c]\n",
    "    ])\n",
    "    return jGz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it working properly? **Try it!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jGz:\n",
      "[[ 0.93937271 -0.41147737]\n",
      " [ 0.34289781  1.12724726]]\n"
     ]
    }
   ],
   "source": [
    "# TRY IT!\n",
    "z = np.vstack([1.2,0.35])\n",
    "Xv = np.vstack([2, 2.1, 0])\n",
    "jGz = get_new_landmark_jacobians(Xv, z)\n",
    "print('jGz:\\n' + str(jGz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Expected output:</span>\n",
    "\n",
    "```\n",
    "jGz:\n",
    "[[ 0.93937271 -0.41147737]\n",
    " [ 0.34289781  1.12724726]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incorporate_new_landmark(robot: EFKMappingRobot, z, iLandmark, xPred, PPred):\n",
    "    \"\"\" Incorporates the information relative to a new ladmark to our system\n",
    "        robot: Robot base (contains state map: xEst, PEst)\n",
    "        z: Observation of a landmark\n",
    "        iLandmark: Index of z in the world map\n",
    "        xPred: Predicted map\n",
    "        PPred: Uncertainty of the prediction\n",
    "\n",
    "        Returns: Nothing. But it modifies the state in robot\n",
    "    \"\"\"\n",
    "    # This is a new feature, so add it to the map        \n",
    "        \n",
    "    # The observation is in the local frame of the robot, it has to\n",
    "    # be translated to the global frame\n",
    "    a = robot.true_pose[2,0] + z[1,0]\n",
    "    r, s, c = z[0,0], np.sin(a), np.cos(a)\n",
    "    xLandmark = np.array([[robot.true_pose[0,0] + r*c],\n",
    "                          [robot.true_pose[1,0] + r*s]])\n",
    "\n",
    "    # Add it to the current state\n",
    "    nStates = xPred.size \n",
    "        \n",
    "    if nStates == 0:\n",
    "        robot.xEst = xLandmark\n",
    "    else:\n",
    "        robot.xEst = np.vstack([robot.xEst, xLandmark]) #Each new feature two new rows\n",
    "\n",
    "    # Compute the jacobian\n",
    "    jGz = get_new_landmark_jacobians(robot.true_pose, z) #Dimension 2x2\n",
    "    \n",
    "    # Build a matrix M incorporating the jacobian to multiply the extendend PEst matrix by it (see below)\n",
    "    if nStates != 0:\n",
    "        # note we don't use jacobian w.r.t vehicle since the pose doesn’t have uncertainty\n",
    "        M = np.vstack([  \n",
    "                np.hstack([jGz, np.zeros((nStates, 2))]),\n",
    "                np.hstack([np.zeros((2, nStates)), jGz])\n",
    "            ])\n",
    "    else: \n",
    "        # First landmark observed!\n",
    "        M = jGz\n",
    "\n",
    "    # Hago la otra forma indicada por el profesor en el comentario de abajo\n",
    "    new_sigma_wc = jGz@ robot.QEst @jGz.T\n",
    "    robot.PEst = linalg.block_diag(robot.PEst, new_sigma_wc)\n",
    "    # robot.PEst = M@linalg.block_diag(None, PPred)@M.T\n",
    "\n",
    "    #This can also be done directly PEst = [PEst,zeros(nStates,2);\n",
    "                                            # zeros(2,nStates),\n",
    "                                            # jGz*QEst*jGz']\n",
    "\n",
    "    #remember this landmark as being mapped: we store its ID for the state vector\n",
    "    robot.MappedLandmarks[iLandmark] = robot.xEst.size-2 #Always an odd number\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The correction (update) step\n",
    "\n",
    "Once a landmark has been detected and its provided information (location and uncertainty) has been properly incorporated to our mapping system, such an information can be updated with new measurements of such a landmark. For doing so, the EKF algorithm performs the following steps:\n",
    "\n",
    "$$\n",
    "  \\begin{aligned}     \n",
    "      & \\textbf{Correction.} \\\\\n",
    "      & K_t = \\bar\\Sigma_t H^T_t (H_t \\bar\\Sigma_t H^T_t + Q_t)^{-1} &\\text{(3. Kalman gain)}\\\\\n",
    "      & m_t = \\bar m_t + K_t (z_t - h(x_t,\\bar m_t)) &\\text{(4. Map estimation)}\\\\\n",
    "      & \\Sigma_t = (I - K_t H_t) \\bar\\Sigma_t &\\text{(5. Uncertainty of estimation)}\\\\\n",
    "      & \\verb!return ! m_t, \\Sigma_t\n",
    "  \\end{aligned}\n",
    "$$\n",
    "\n",
    "Notice that the map (landmark locations) is estimated according to the map estimation in the previous time step $t-1$, the *Kalman gain*, and the error (also called ***innovation***) between the obsevation taken by the sensor ($z_t$) and the one computed by the observation model given the robot pose and the predicted map, that is $z_t - h(x_t,\\bar m_t$).\n",
    "\n",
    "In such equations, H (``jH`` in our code) represents the jacobian of the observation model. The shape of such a jacobian is $(2,M\\times2)$, and has the following form:\n",
    "\n",
    "$$\n",
    "H = \\left[\n",
    "  \\begin{matrix}\n",
    "0 & 0 & \\cdots \\\\\n",
    "0 & 0 & \\cdots \\\\\n",
    "\\end{matrix}\n",
    "\\right .\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "jHxl_{11} & jHxl_{12} \\\\\n",
    "jHxl_{21} & jHxl_{22} \\\\\n",
    "\\end{bmatrix}}_\\text{Jacobian for the observed landmark, $jHxl$}\n",
    "\\left .\n",
    "\\begin{matrix}\n",
    "\\cdots & 0 & 0 \\\\\n",
    "\\cdots & 0 & 0 \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "jHxl = \n",
    "\\begin{bmatrix}\n",
    "(x_l - x)/d & (y_l - y)/d \\\\\n",
    "-(y_l - y)/d^2 & (x_l - x)/d^2 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $[x_l,y_l]$ is the position of the landmark, $[x,y]$ is the robot location, and $d=\\sqrt{(x_l-x)^2 + (y_l-y)^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 3: Updating the knowledge about an observed landmark</i></b></span>** \n",
    "\n",
    "**Your job at this point is:**\n",
    "\n",
    "- To implement the function ``get_observation_jacobian()`` returning the jacobian of the observed landmark, that is, $jHxl$.\n",
    "- To complete the ``update_step()`` method that performs the update step of the EKF algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_observation_jacobian(xPred, xLandmark):\n",
    "    \"\"\" Calculate the jacobian of the observation model.\n",
    "        \n",
    "        Needed to update a landmark we have already seen.\n",
    "        Hint. Similar to the one described in unit 5 (Localization)\n",
    "        \n",
    "        Args:\n",
    "            xPred: True pose of our robot.\n",
    "            xLandmark: Estimated pose of a landmark in our map. World p.o.v in cartesian coordinates.\n",
    "                Does not contain an angle.\n",
    "        \n",
    "        Return:\n",
    "            jHxl: 2x2 matrix containing the corresponding jacobian.\n",
    "    \"\"\"\n",
    "    xdist = xLandmark[0,0] - xPred[0,0]\n",
    "    ydist = xLandmark[1,0] - xPred[1,0]\n",
    "    r = np.sqrt(xdist**2 + ydist**2)\n",
    "    r2 = r**2\n",
    "    jHxl = np.array([\n",
    "        [xdist/r , ydist/r],\n",
    "        [-ydist/r2 , xdist/r2]\n",
    "    ])\n",
    "    return jHxl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if your implementation is right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jHxl:\n",
      "[[ 0.98058068 -0.19611614]\n",
      " [ 0.38461538  1.92307692]]\n"
     ]
    }
   ],
   "source": [
    "# TRY IT!\n",
    "xLandmark = np.vstack([2.5,2])\n",
    "xPred = np.vstack([2, 2.1, 0])\n",
    "jHxl = get_observation_jacobian(xPred, xLandmark)\n",
    "print('jHxl:\\n' + str(jHxl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Expected output:</span>\n",
    "\n",
    "```\n",
    "jHxl:\n",
    "[[ 0.98058068 -0.19611614]\n",
    " [ 0.38461538  1.92307692]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_step(robot, z, iLandmark, xPred, PPred):\n",
    "    \"\"\" Performs the update step of EKF\n",
    "        robot: Robot base (contains state map: xEst, PEst)\n",
    "        z: Observation of a landmark\n",
    "        iLandmark: Index of z in the world map\n",
    "        xPred: Predicted map\n",
    "        PPred: Uncertainty of the prediction\n",
    "        \n",
    "        Returns: Nothing. But it modifies the state in robot\n",
    "    \"\"\"\n",
    "    # Find out where it is in state vector\n",
    "    landmarkIndex = robot.MappedLandmarks[iLandmark[0], 0]\n",
    "        \n",
    "    # xLandmark is the current estimation of the position of the\n",
    "    # landmard \"FeatureIndex\"\n",
    "    xLandmark = xPred[landmarkIndex:landmarkIndex+2]\n",
    "        \n",
    "    # DONE Predicts the observation\n",
    "    zPred = robot.observe(0, xLandmark, noisy=False) # Hint: use robot.observe function\n",
    "\n",
    "    # Get observation Jacobians\n",
    "    jHxf = get_observation_jacobian(xPred, zPred)\n",
    "        \n",
    "    # Fill in state jacobian\n",
    "    # (the jacobian is zero except for the observed landmark)\n",
    "    jH = np.zeros((2, xPred.size))\n",
    "    jH[:, landmarkIndex:landmarkIndex+2] = jHxf\n",
    "        \n",
    "    #\n",
    "    # Kalman update\n",
    "    #\n",
    "    # robot.Q, or robot.QEst ??\n",
    "    Innov = z-xLandmark # LOOKS GOOD # Innovation\n",
    "    Innov[1] = AngleWrap(Innov[1])\n",
    "    S = jH@PPred@jH.T + robot.Q\n",
    "    K = PPred@jH@linalg.inv(S) # LOOKS GOOD # 3. Gain\n",
    "    robot.xEst = xPred + K@Innov # 4. Map estimation    \n",
    "    robot.PEst = (np.eye(robot.PEst.shape[0]) - K@jH)@PPred\n",
    "    #robot.PEst = PPred - K@S@K.T # Alternative way\n",
    "    \n",
    "    #ensure P remains symmetric\n",
    "    robot.PEst = 0.5*(robot.PEst+robot.PEst.T)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 4: Putting all together in the EKF algorithm</i></b></span>** \n",
    "\n",
    "Now that you have implemented the building blocks of the EKF filter for mapping the **<span style=\"color:seagreen\">Nirvana shopping mall</span>**, it is time to write a simple function ``EKFMapping()`` putting them together. For that, **you have to** call each method with the appropriate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EKFMapping(robot: EFKMappingRobot, z, iLandmark):\n",
    "    \"\"\" EFK algorithm for mapping\n",
    "        \n",
    "            robot: Robot base (contains state map: xEst, PEst)\n",
    "            z: Observation of a landmark\n",
    "            iLandmark: Index of z in the world map\n",
    "    \n",
    "        Returns: Nothing. But it modifies the state in robot\n",
    "    \"\"\"\n",
    "    \n",
    "    # Do prediction step\n",
    "    [xPred, PPred] = prediction_step(robot)\n",
    "\n",
    "    # Check if feature observed is in map\n",
    "    if robot.MappedLandmarks[iLandmark] > -1:\n",
    "        update_step(robot, z, iLandmark, xPred, PPred)\n",
    "        \n",
    "    else:\n",
    "        # This is a new feature, so add its information to the map        \n",
    "        incorporate_new_landmark(robot, z, iLandmark, xPred, PPred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.3 Testing the mapping system\n",
    "\n",
    "### Playing with one landmark\n",
    "\n",
    "Let's consider that the mall has only one landmark to get things started (``nLandmarks=1``). The following function provides a demo where the robot is commanded to follow a squared trajectory while observing a landmark after each movement. \n",
    "\n",
    "The **<span style=\"color:seagreen\">Nirvana</span>** managers are curious about the state and dimensions of the variables storing the estimated positions `xEst` and their associated uncertainties `Pest`, so we show their content after each 5 iterations of the algorithm. \n",
    "\n",
    "<figure style=\"text-align:center\">\n",
    "  <img src=\"images/fig6-1-2.png\" width=\"500\" alt=\"\">\n",
    "  <figcaption>\n",
    "      Fig. 1: Example run of the EKF algorithmn for mapping (only one landmark). <br/>\n",
    "      it shows the true pose (in red), <br/>\n",
    "      the real pose of the landmark (as a green star), <br/>\n",
    "      and the estimation from the EKF algorithm (pose and confidence ellipse).\n",
    "  </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_ekf_mapping(robot,\n",
    "                     Map,\n",
    "                     nLandmarks,\n",
    "                     mode='non_stop',\n",
    "                     logger=None,\n",
    "                     nSteps=100, # Number of motions\n",
    "                     turning= 40, # Number of motions before turning (square path)\n",
    "                     print_each=2):\n",
    "    \n",
    "    %matplotlib widget\n",
    "    if mode == 'step_by_step':\n",
    "        matplotlib.use('TkAgg')\n",
    "\n",
    "    # storing the number of times a landmark has been seen\n",
    "    # also store the handler to the graphical info shown\n",
    "    canvas = MapCanvas(nLandmarks)\n",
    "    \n",
    "    canvas.ax.plot(Map[0, :], Map[1, :], 'g*')\n",
    "    hObsLine = canvas.ax.plot([0,0], [0,0], linestyle=':')\n",
    "    \n",
    "    # Control action\n",
    "    u = np.zeros((3, 1))\n",
    "    u[0] = (2.*MapSize/1.5)/turning\n",
    "    u[1] = 0.\n",
    "    \n",
    "    log_xEst = []\n",
    "    log_PEst = []\n",
    "    \n",
    "    # Start the loop!\n",
    "    for k in range(nSteps):\n",
    "        #\n",
    "        # Move the robot\n",
    "        #\n",
    "        u[2]=0.\n",
    "        if k%turning == turning-1:\n",
    "            u[2] = np.pi/2\n",
    "        \n",
    "        robot.step(u) # Perfectly known robot pose\n",
    "        \n",
    "        z, iLandmark = robot.get_random_observation(world=Map)\n",
    "        \n",
    "        # Update the \"observedtimes\" for the feature and plot the reading\n",
    "        canvas.increment_observed_times(iLandmark)\n",
    "        canvas.PlotNumberOfReadings(robot.true_pose, iLandmark, Map)\n",
    "        \n",
    "        EKFMapping(robot, z, iLandmark)\n",
    "        \n",
    "        # Store map evolution each 5 steps\n",
    "        if not k%5:\n",
    "            log_xEst.append(robot.xEst)\n",
    "            log_PEst.append(robot.PEst)\n",
    "        \n",
    "        # Log important values\n",
    "        if logger is not None:\n",
    "            logger.log(k, robot, Map)\n",
    "        \n",
    "        # Drawings\n",
    "        if k%print_each == print_each-1:\n",
    "            DrawRobot(canvas.fig, canvas.ax,robot.true_pose, 'r')#plot(xVehicleTrue(1),xVehicleTrue(2),'r*')\n",
    "            canvas.DoMapGraphics(robot) # Draw estimated poitns (in black) and ellipses\n",
    "            plt.axis([-MapSize-5, MapSize+5, -MapSize-5, MapSize+5]) # Set limits again\n",
    "            #plt.draw()\n",
    "#           plt.savefig(str(k)+'.jpg')\n",
    "            clear_output(wait=True)\n",
    "            display(canvas.fig) \n",
    "            \n",
    "            if mode == 'step_by_step':\n",
    "                plt.waitforbuttonpress(-1)\n",
    "            elif mode == 'visualize_process':\n",
    "                time.sleep(0.2)\n",
    "            elif mode == 'non_stop':\n",
    "                pass # non stop!\n",
    "\n",
    "    # Final drawings\n",
    "    %matplotlib inline\n",
    "    if logger is not None:\n",
    "        logger.plot()\n",
    "    \n",
    "    # Print map evolution each 5 steps\n",
    "    for i in range(0,len(log_xEst)):\n",
    "        with np.printoptions(precision=3):\n",
    "            print('Iteration: ' + str(i*5))\n",
    "            print('Estimated xEst:\\n' + str(log_xEst[i]))\n",
    "            print('Estimated PEst:\\n' + str(log_PEst[i]))\n",
    "            print('---------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Cannot change to a different GUI toolkit: widget. Using notebook instead.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "/* Put everything inside the global mpl namespace */\n/* global mpl */\nwindow.mpl = {};\n\nmpl.get_websocket_type = function () {\n    if (typeof WebSocket !== 'undefined') {\n        return WebSocket;\n    } else if (typeof MozWebSocket !== 'undefined') {\n        return MozWebSocket;\n    } else {\n        alert(\n            'Your browser does not have WebSocket support. ' +\n                'Please try Chrome, Safari or Firefox ≥ 6. ' +\n                'Firefox 4 and 5 are also supported but you ' +\n                'have to enable WebSockets in about:config.'\n        );\n    }\n};\n\nmpl.figure = function (figure_id, websocket, ondownload, parent_element) {\n    this.id = figure_id;\n\n    this.ws = websocket;\n\n    this.supports_binary = this.ws.binaryType !== undefined;\n\n    if (!this.supports_binary) {\n        var warnings = document.getElementById('mpl-warnings');\n        if (warnings) {\n            warnings.style.display = 'block';\n            warnings.textContent =\n                'This browser does not support binary websocket messages. ' +\n                'Performance may be slow.';\n        }\n    }\n\n    this.imageObj = new Image();\n\n    this.context = undefined;\n    this.message = undefined;\n    this.canvas = undefined;\n    this.rubberband_canvas = undefined;\n    this.rubberband_context = undefined;\n    this.format_dropdown = undefined;\n\n    this.image_mode = 'full';\n\n    this.root = document.createElement('div');\n    this.root.setAttribute('style', 'display: inline-block');\n    this._root_extra_style(this.root);\n\n    parent_element.appendChild(this.root);\n\n    this._init_header(this);\n    this._init_canvas(this);\n    this._init_toolbar(this);\n\n    var fig = this;\n\n    this.waiting = false;\n\n    this.ws.onopen = function () {\n        fig.send_message('supports_binary', { value: fig.supports_binary });\n        fig.send_message('send_image_mode', {});\n        if (fig.ratio !== 1) {\n            fig.send_message('set_device_pixel_ratio', {\n                device_pixel_ratio: fig.ratio,\n            });\n        }\n        fig.send_message('refresh', {});\n    };\n\n    this.imageObj.onload = function () {\n        if (fig.image_mode === 'full') {\n            // Full images could contain transparency (where diff images\n            // almost always do), so we need to clear the canvas so that\n            // there is no ghosting.\n            fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n        }\n        fig.context.drawImage(fig.imageObj, 0, 0);\n    };\n\n    this.imageObj.onunload = function () {\n        fig.ws.close();\n    };\n\n    this.ws.onmessage = this._make_on_message_function(this);\n\n    this.ondownload = ondownload;\n};\n\nmpl.figure.prototype._init_header = function () {\n    var titlebar = document.createElement('div');\n    titlebar.classList =\n        'ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix';\n    var titletext = document.createElement('div');\n    titletext.classList = 'ui-dialog-title';\n    titletext.setAttribute(\n        'style',\n        'width: 100%; text-align: center; padding: 3px;'\n    );\n    titlebar.appendChild(titletext);\n    this.root.appendChild(titlebar);\n    this.header = titletext;\n};\n\nmpl.figure.prototype._canvas_extra_style = function (_canvas_div) {};\n\nmpl.figure.prototype._root_extra_style = function (_canvas_div) {};\n\nmpl.figure.prototype._init_canvas = function () {\n    var fig = this;\n\n    var canvas_div = (this.canvas_div = document.createElement('div'));\n    canvas_div.setAttribute(\n        'style',\n        'border: 1px solid #ddd;' +\n            'box-sizing: content-box;' +\n            'clear: both;' +\n            'min-height: 1px;' +\n            'min-width: 1px;' +\n            'outline: 0;' +\n            'overflow: hidden;' +\n            'position: relative;' +\n            'resize: both;'\n    );\n\n    function on_keyboard_event_closure(name) {\n        return function (event) {\n            return fig.key_event(event, name);\n        };\n    }\n\n    canvas_div.addEventListener(\n        'keydown',\n        on_keyboard_event_closure('key_press')\n    );\n    canvas_div.addEventListener(\n        'keyup',\n        on_keyboard_event_closure('key_release')\n    );\n\n    this._canvas_extra_style(canvas_div);\n    this.root.appendChild(canvas_div);\n\n    var canvas = (this.canvas = document.createElement('canvas'));\n    canvas.classList.add('mpl-canvas');\n    canvas.setAttribute('style', 'box-sizing: content-box;');\n\n    this.context = canvas.getContext('2d');\n\n    var backingStore =\n        this.context.backingStorePixelRatio ||\n        this.context.webkitBackingStorePixelRatio ||\n        this.context.mozBackingStorePixelRatio ||\n        this.context.msBackingStorePixelRatio ||\n        this.context.oBackingStorePixelRatio ||\n        this.context.backingStorePixelRatio ||\n        1;\n\n    this.ratio = (window.devicePixelRatio || 1) / backingStore;\n\n    var rubberband_canvas = (this.rubberband_canvas = document.createElement(\n        'canvas'\n    ));\n    rubberband_canvas.setAttribute(\n        'style',\n        'box-sizing: content-box; position: absolute; left: 0; top: 0; z-index: 1;'\n    );\n\n    // Apply a ponyfill if ResizeObserver is not implemented by browser.\n    if (this.ResizeObserver === undefined) {\n        if (window.ResizeObserver !== undefined) {\n            this.ResizeObserver = window.ResizeObserver;\n        } else {\n            var obs = _JSXTOOLS_RESIZE_OBSERVER({});\n            this.ResizeObserver = obs.ResizeObserver;\n        }\n    }\n\n    this.resizeObserverInstance = new this.ResizeObserver(function (entries) {\n        var nentries = entries.length;\n        for (var i = 0; i < nentries; i++) {\n            var entry = entries[i];\n            var width, height;\n            if (entry.contentBoxSize) {\n                if (entry.contentBoxSize instanceof Array) {\n                    // Chrome 84 implements new version of spec.\n                    width = entry.contentBoxSize[0].inlineSize;\n                    height = entry.contentBoxSize[0].blockSize;\n                } else {\n                    // Firefox implements old version of spec.\n                    width = entry.contentBoxSize.inlineSize;\n                    height = entry.contentBoxSize.blockSize;\n                }\n            } else {\n                // Chrome <84 implements even older version of spec.\n                width = entry.contentRect.width;\n                height = entry.contentRect.height;\n            }\n\n            // Keep the size of the canvas and rubber band canvas in sync with\n            // the canvas container.\n            if (entry.devicePixelContentBoxSize) {\n                // Chrome 84 implements new version of spec.\n                canvas.setAttribute(\n                    'width',\n                    entry.devicePixelContentBoxSize[0].inlineSize\n                );\n                canvas.setAttribute(\n                    'height',\n                    entry.devicePixelContentBoxSize[0].blockSize\n                );\n            } else {\n                canvas.setAttribute('width', width * fig.ratio);\n                canvas.setAttribute('height', height * fig.ratio);\n            }\n            canvas.setAttribute(\n                'style',\n                'width: ' + width + 'px; height: ' + height + 'px;'\n            );\n\n            rubberband_canvas.setAttribute('width', width);\n            rubberband_canvas.setAttribute('height', height);\n\n            // And update the size in Python. We ignore the initial 0/0 size\n            // that occurs as the element is placed into the DOM, which should\n            // otherwise not happen due to the minimum size styling.\n            if (fig.ws.readyState == 1 && width != 0 && height != 0) {\n                fig.request_resize(width, height);\n            }\n        }\n    });\n    this.resizeObserverInstance.observe(canvas_div);\n\n    function on_mouse_event_closure(name) {\n        return function (event) {\n            return fig.mouse_event(event, name);\n        };\n    }\n\n    rubberband_canvas.addEventListener(\n        'mousedown',\n        on_mouse_event_closure('button_press')\n    );\n    rubberband_canvas.addEventListener(\n        'mouseup',\n        on_mouse_event_closure('button_release')\n    );\n    rubberband_canvas.addEventListener(\n        'dblclick',\n        on_mouse_event_closure('dblclick')\n    );\n    // Throttle sequential mouse events to 1 every 20ms.\n    rubberband_canvas.addEventListener(\n        'mousemove',\n        on_mouse_event_closure('motion_notify')\n    );\n\n    rubberband_canvas.addEventListener(\n        'mouseenter',\n        on_mouse_event_closure('figure_enter')\n    );\n    rubberband_canvas.addEventListener(\n        'mouseleave',\n        on_mouse_event_closure('figure_leave')\n    );\n\n    canvas_div.addEventListener('wheel', function (event) {\n        if (event.deltaY < 0) {\n            event.step = 1;\n        } else {\n            event.step = -1;\n        }\n        on_mouse_event_closure('scroll')(event);\n    });\n\n    canvas_div.appendChild(canvas);\n    canvas_div.appendChild(rubberband_canvas);\n\n    this.rubberband_context = rubberband_canvas.getContext('2d');\n    this.rubberband_context.strokeStyle = '#000000';\n\n    this._resize_canvas = function (width, height, forward) {\n        if (forward) {\n            canvas_div.style.width = width + 'px';\n            canvas_div.style.height = height + 'px';\n        }\n    };\n\n    // Disable right mouse context menu.\n    this.rubberband_canvas.addEventListener('contextmenu', function (_e) {\n        event.preventDefault();\n        return false;\n    });\n\n    function set_focus() {\n        canvas.focus();\n        canvas_div.focus();\n    }\n\n    window.setTimeout(set_focus, 100);\n};\n\nmpl.figure.prototype._init_toolbar = function () {\n    var fig = this;\n\n    var toolbar = document.createElement('div');\n    toolbar.classList = 'mpl-toolbar';\n    this.root.appendChild(toolbar);\n\n    function on_click_closure(name) {\n        return function (_event) {\n            return fig.toolbar_button_onclick(name);\n        };\n    }\n\n    function on_mouseover_closure(tooltip) {\n        return function (event) {\n            if (!event.currentTarget.disabled) {\n                return fig.toolbar_button_onmouseover(tooltip);\n            }\n        };\n    }\n\n    fig.buttons = {};\n    var buttonGroup = document.createElement('div');\n    buttonGroup.classList = 'mpl-button-group';\n    for (var toolbar_ind in mpl.toolbar_items) {\n        var name = mpl.toolbar_items[toolbar_ind][0];\n        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n        var image = mpl.toolbar_items[toolbar_ind][2];\n        var method_name = mpl.toolbar_items[toolbar_ind][3];\n\n        if (!name) {\n            /* Instead of a spacer, we start a new button group. */\n            if (buttonGroup.hasChildNodes()) {\n                toolbar.appendChild(buttonGroup);\n            }\n            buttonGroup = document.createElement('div');\n            buttonGroup.classList = 'mpl-button-group';\n            continue;\n        }\n\n        var button = (fig.buttons[name] = document.createElement('button'));\n        button.classList = 'mpl-widget';\n        button.setAttribute('role', 'button');\n        button.setAttribute('aria-disabled', 'false');\n        button.addEventListener('click', on_click_closure(method_name));\n        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n\n        var icon_img = document.createElement('img');\n        icon_img.src = '_images/' + image + '.png';\n        icon_img.srcset = '_images/' + image + '_large.png 2x';\n        icon_img.alt = tooltip;\n        button.appendChild(icon_img);\n\n        buttonGroup.appendChild(button);\n    }\n\n    if (buttonGroup.hasChildNodes()) {\n        toolbar.appendChild(buttonGroup);\n    }\n\n    var fmt_picker = document.createElement('select');\n    fmt_picker.classList = 'mpl-widget';\n    toolbar.appendChild(fmt_picker);\n    this.format_dropdown = fmt_picker;\n\n    for (var ind in mpl.extensions) {\n        var fmt = mpl.extensions[ind];\n        var option = document.createElement('option');\n        option.selected = fmt === mpl.default_extension;\n        option.innerHTML = fmt;\n        fmt_picker.appendChild(option);\n    }\n\n    var status_bar = document.createElement('span');\n    status_bar.classList = 'mpl-message';\n    toolbar.appendChild(status_bar);\n    this.message = status_bar;\n};\n\nmpl.figure.prototype.request_resize = function (x_pixels, y_pixels) {\n    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n    // which will in turn request a refresh of the image.\n    this.send_message('resize', { width: x_pixels, height: y_pixels });\n};\n\nmpl.figure.prototype.send_message = function (type, properties) {\n    properties['type'] = type;\n    properties['figure_id'] = this.id;\n    this.ws.send(JSON.stringify(properties));\n};\n\nmpl.figure.prototype.send_draw_message = function () {\n    if (!this.waiting) {\n        this.waiting = true;\n        this.ws.send(JSON.stringify({ type: 'draw', figure_id: this.id }));\n    }\n};\n\nmpl.figure.prototype.handle_save = function (fig, _msg) {\n    var format_dropdown = fig.format_dropdown;\n    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n    fig.ondownload(fig, format);\n};\n\nmpl.figure.prototype.handle_resize = function (fig, msg) {\n    var size = msg['size'];\n    if (size[0] !== fig.canvas.width || size[1] !== fig.canvas.height) {\n        fig._resize_canvas(size[0], size[1], msg['forward']);\n        fig.send_message('refresh', {});\n    }\n};\n\nmpl.figure.prototype.handle_rubberband = function (fig, msg) {\n    var x0 = msg['x0'] / fig.ratio;\n    var y0 = (fig.canvas.height - msg['y0']) / fig.ratio;\n    var x1 = msg['x1'] / fig.ratio;\n    var y1 = (fig.canvas.height - msg['y1']) / fig.ratio;\n    x0 = Math.floor(x0) + 0.5;\n    y0 = Math.floor(y0) + 0.5;\n    x1 = Math.floor(x1) + 0.5;\n    y1 = Math.floor(y1) + 0.5;\n    var min_x = Math.min(x0, x1);\n    var min_y = Math.min(y0, y1);\n    var width = Math.abs(x1 - x0);\n    var height = Math.abs(y1 - y0);\n\n    fig.rubberband_context.clearRect(\n        0,\n        0,\n        fig.canvas.width / fig.ratio,\n        fig.canvas.height / fig.ratio\n    );\n\n    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n};\n\nmpl.figure.prototype.handle_figure_label = function (fig, msg) {\n    // Updates the figure title.\n    fig.header.textContent = msg['label'];\n};\n\nmpl.figure.prototype.handle_cursor = function (fig, msg) {\n    fig.rubberband_canvas.style.cursor = msg['cursor'];\n};\n\nmpl.figure.prototype.handle_message = function (fig, msg) {\n    fig.message.textContent = msg['message'];\n};\n\nmpl.figure.prototype.handle_draw = function (fig, _msg) {\n    // Request the server to send over a new figure.\n    fig.send_draw_message();\n};\n\nmpl.figure.prototype.handle_image_mode = function (fig, msg) {\n    fig.image_mode = msg['mode'];\n};\n\nmpl.figure.prototype.handle_history_buttons = function (fig, msg) {\n    for (var key in msg) {\n        if (!(key in fig.buttons)) {\n            continue;\n        }\n        fig.buttons[key].disabled = !msg[key];\n        fig.buttons[key].setAttribute('aria-disabled', !msg[key]);\n    }\n};\n\nmpl.figure.prototype.handle_navigate_mode = function (fig, msg) {\n    if (msg['mode'] === 'PAN') {\n        fig.buttons['Pan'].classList.add('active');\n        fig.buttons['Zoom'].classList.remove('active');\n    } else if (msg['mode'] === 'ZOOM') {\n        fig.buttons['Pan'].classList.remove('active');\n        fig.buttons['Zoom'].classList.add('active');\n    } else {\n        fig.buttons['Pan'].classList.remove('active');\n        fig.buttons['Zoom'].classList.remove('active');\n    }\n};\n\nmpl.figure.prototype.updated_canvas_event = function () {\n    // Called whenever the canvas gets updated.\n    this.send_message('ack', {});\n};\n\n// A function to construct a web socket function for onmessage handling.\n// Called in the figure constructor.\nmpl.figure.prototype._make_on_message_function = function (fig) {\n    return function socket_on_message(evt) {\n        if (evt.data instanceof Blob) {\n            var img = evt.data;\n            if (img.type !== 'image/png') {\n                /* FIXME: We get \"Resource interpreted as Image but\n                 * transferred with MIME type text/plain:\" errors on\n                 * Chrome.  But how to set the MIME type?  It doesn't seem\n                 * to be part of the websocket stream */\n                img.type = 'image/png';\n            }\n\n            /* Free the memory for the previous frames */\n            if (fig.imageObj.src) {\n                (window.URL || window.webkitURL).revokeObjectURL(\n                    fig.imageObj.src\n                );\n            }\n\n            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n                img\n            );\n            fig.updated_canvas_event();\n            fig.waiting = false;\n            return;\n        } else if (\n            typeof evt.data === 'string' &&\n            evt.data.slice(0, 21) === 'data:image/png;base64'\n        ) {\n            fig.imageObj.src = evt.data;\n            fig.updated_canvas_event();\n            fig.waiting = false;\n            return;\n        }\n\n        var msg = JSON.parse(evt.data);\n        var msg_type = msg['type'];\n\n        // Call the  \"handle_{type}\" callback, which takes\n        // the figure and JSON message as its only arguments.\n        try {\n            var callback = fig['handle_' + msg_type];\n        } catch (e) {\n            console.log(\n                \"No handler for the '\" + msg_type + \"' message type: \",\n                msg\n            );\n            return;\n        }\n\n        if (callback) {\n            try {\n                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n                callback(fig, msg);\n            } catch (e) {\n                console.log(\n                    \"Exception inside the 'handler_\" + msg_type + \"' callback:\",\n                    e,\n                    e.stack,\n                    msg\n                );\n            }\n        }\n    };\n};\n\n// from https://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\nmpl.findpos = function (e) {\n    //this section is from http://www.quirksmode.org/js/events_properties.html\n    var targ;\n    if (!e) {\n        e = window.event;\n    }\n    if (e.target) {\n        targ = e.target;\n    } else if (e.srcElement) {\n        targ = e.srcElement;\n    }\n    if (targ.nodeType === 3) {\n        // defeat Safari bug\n        targ = targ.parentNode;\n    }\n\n    // pageX,Y are the mouse positions relative to the document\n    var boundingRect = targ.getBoundingClientRect();\n    var x = e.pageX - (boundingRect.left + document.body.scrollLeft);\n    var y = e.pageY - (boundingRect.top + document.body.scrollTop);\n\n    return { x: x, y: y };\n};\n\n/*\n * return a copy of an object with only non-object keys\n * we need this to avoid circular references\n * https://stackoverflow.com/a/24161582/3208463\n */\nfunction simpleKeys(original) {\n    return Object.keys(original).reduce(function (obj, key) {\n        if (typeof original[key] !== 'object') {\n            obj[key] = original[key];\n        }\n        return obj;\n    }, {});\n}\n\nmpl.figure.prototype.mouse_event = function (event, name) {\n    var canvas_pos = mpl.findpos(event);\n\n    if (name === 'button_press') {\n        this.canvas.focus();\n        this.canvas_div.focus();\n    }\n\n    var x = canvas_pos.x * this.ratio;\n    var y = canvas_pos.y * this.ratio;\n\n    this.send_message(name, {\n        x: x,\n        y: y,\n        button: event.button,\n        step: event.step,\n        guiEvent: simpleKeys(event),\n    });\n\n    /* This prevents the web browser from automatically changing to\n     * the text insertion cursor when the button is pressed.  We want\n     * to control all of the cursor setting manually through the\n     * 'cursor' event from matplotlib */\n    event.preventDefault();\n    return false;\n};\n\nmpl.figure.prototype._key_event_extra = function (_event, _name) {\n    // Handle any extra behaviour associated with a key event\n};\n\nmpl.figure.prototype.key_event = function (event, name) {\n    // Prevent repeat events\n    if (name === 'key_press') {\n        if (event.key === this._key) {\n            return;\n        } else {\n            this._key = event.key;\n        }\n    }\n    if (name === 'key_release') {\n        this._key = null;\n    }\n\n    var value = '';\n    if (event.ctrlKey && event.key !== 'Control') {\n        value += 'ctrl+';\n    }\n    else if (event.altKey && event.key !== 'Alt') {\n        value += 'alt+';\n    }\n    else if (event.shiftKey && event.key !== 'Shift') {\n        value += 'shift+';\n    }\n\n    value += 'k' + event.key;\n\n    this._key_event_extra(event, name);\n\n    this.send_message(name, { key: value, guiEvent: simpleKeys(event) });\n    return false;\n};\n\nmpl.figure.prototype.toolbar_button_onclick = function (name) {\n    if (name === 'download') {\n        this.handle_save(this, null);\n    } else {\n        this.send_message('toolbar_button', { name: name });\n    }\n};\n\nmpl.figure.prototype.toolbar_button_onmouseover = function (tooltip) {\n    this.message.textContent = tooltip;\n};\n\n///////////////// REMAINING CONTENT GENERATED BY embed_js.py /////////////////\n// prettier-ignore\nvar _JSXTOOLS_RESIZE_OBSERVER=function(A){var t,i=new WeakMap,n=new WeakMap,a=new WeakMap,r=new WeakMap,o=new Set;function s(e){if(!(this instanceof s))throw new TypeError(\"Constructor requires 'new' operator\");i.set(this,e)}function h(){throw new TypeError(\"Function is not a constructor\")}function c(e,t,i,n){e=0 in arguments?Number(arguments[0]):0,t=1 in arguments?Number(arguments[1]):0,i=2 in arguments?Number(arguments[2]):0,n=3 in arguments?Number(arguments[3]):0,this.right=(this.x=this.left=e)+(this.width=i),this.bottom=(this.y=this.top=t)+(this.height=n),Object.freeze(this)}function d(){t=requestAnimationFrame(d);var s=new WeakMap,p=new Set;o.forEach((function(t){r.get(t).forEach((function(i){var r=t instanceof window.SVGElement,o=a.get(t),d=r?0:parseFloat(o.paddingTop),f=r?0:parseFloat(o.paddingRight),l=r?0:parseFloat(o.paddingBottom),u=r?0:parseFloat(o.paddingLeft),g=r?0:parseFloat(o.borderTopWidth),m=r?0:parseFloat(o.borderRightWidth),w=r?0:parseFloat(o.borderBottomWidth),b=u+f,F=d+l,v=(r?0:parseFloat(o.borderLeftWidth))+m,W=g+w,y=r?0:t.offsetHeight-W-t.clientHeight,E=r?0:t.offsetWidth-v-t.clientWidth,R=b+v,z=F+W,M=r?t.width:parseFloat(o.width)-R-E,O=r?t.height:parseFloat(o.height)-z-y;if(n.has(t)){var k=n.get(t);if(k[0]===M&&k[1]===O)return}n.set(t,[M,O]);var S=Object.create(h.prototype);S.target=t,S.contentRect=new c(u,d,M,O),s.has(i)||(s.set(i,[]),p.add(i)),s.get(i).push(S)}))})),p.forEach((function(e){i.get(e).call(e,s.get(e),e)}))}return s.prototype.observe=function(i){if(i instanceof window.Element){r.has(i)||(r.set(i,new Set),o.add(i),a.set(i,window.getComputedStyle(i)));var n=r.get(i);n.has(this)||n.add(this),cancelAnimationFrame(t),t=requestAnimationFrame(d)}},s.prototype.unobserve=function(i){if(i instanceof window.Element&&r.has(i)){var n=r.get(i);n.has(this)&&(n.delete(this),n.size||(r.delete(i),o.delete(i))),n.size||r.delete(i),o.size||cancelAnimationFrame(t)}},A.DOMRectReadOnly=c,A.ResizeObserver=s,A.ResizeObserverEntry=h,A}; // eslint-disable-line\nmpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Left button pans, Right button zooms\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-arrows\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\\nx/y fixes axis\", \"fa fa-square-o\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o\", \"download\"]];\n\nmpl.extensions = [\"eps\", \"jpeg\", \"pgf\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\", \"webp\"];\n\nmpl.default_extension = \"png\";/* global mpl */\n\nvar comm_websocket_adapter = function (comm) {\n    // Create a \"websocket\"-like object which calls the given IPython comm\n    // object with the appropriate methods. Currently this is a non binary\n    // socket, so there is still some room for performance tuning.\n    var ws = {};\n\n    ws.binaryType = comm.kernel.ws.binaryType;\n    ws.readyState = comm.kernel.ws.readyState;\n    function updateReadyState(_event) {\n        if (comm.kernel.ws) {\n            ws.readyState = comm.kernel.ws.readyState;\n        } else {\n            ws.readyState = 3; // Closed state.\n        }\n    }\n    comm.kernel.ws.addEventListener('open', updateReadyState);\n    comm.kernel.ws.addEventListener('close', updateReadyState);\n    comm.kernel.ws.addEventListener('error', updateReadyState);\n\n    ws.close = function () {\n        comm.close();\n    };\n    ws.send = function (m) {\n        //console.log('sending', m);\n        comm.send(m);\n    };\n    // Register the callback with on_msg.\n    comm.on_msg(function (msg) {\n        //console.log('receiving', msg['content']['data'], msg);\n        var data = msg['content']['data'];\n        if (data['blob'] !== undefined) {\n            data = {\n                data: new Blob(msg['buffers'], { type: data['blob'] }),\n            };\n        }\n        // Pass the mpl event to the overridden (by mpl) onmessage function.\n        ws.onmessage(data);\n    });\n    return ws;\n};\n\nmpl.mpl_figure_comm = function (comm, msg) {\n    // This is the function which gets called when the mpl process\n    // starts-up an IPython Comm through the \"matplotlib\" channel.\n\n    var id = msg.content.data.id;\n    // Get hold of the div created by the display call when the Comm\n    // socket was opened in Python.\n    var element = document.getElementById(id);\n    var ws_proxy = comm_websocket_adapter(comm);\n\n    function ondownload(figure, _format) {\n        window.open(figure.canvas.toDataURL());\n    }\n\n    var fig = new mpl.figure(id, ws_proxy, ondownload, element);\n\n    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n    // web socket which is closed, not our websocket->open comm proxy.\n    ws_proxy.onopen();\n\n    fig.parent_element = element;\n    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n    if (!fig.cell_info) {\n        console.error('Failed to find cell for figure', id, fig);\n        return;\n    }\n    fig.cell_info[0].output_area.element.on(\n        'cleared',\n        { fig: fig },\n        fig._remove_fig_handler\n    );\n};\n\nmpl.figure.prototype.handle_close = function (fig, msg) {\n    var width = fig.canvas.width / fig.ratio;\n    fig.cell_info[0].output_area.element.off(\n        'cleared',\n        fig._remove_fig_handler\n    );\n    fig.resizeObserverInstance.unobserve(fig.canvas_div);\n\n    // Update the output cell to use the data from the current canvas.\n    fig.push_to_output();\n    var dataURL = fig.canvas.toDataURL();\n    // Re-enable the keyboard manager in IPython - without this line, in FF,\n    // the notebook keyboard shortcuts fail.\n    IPython.keyboard_manager.enable();\n    fig.parent_element.innerHTML =\n        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n    fig.close_ws(fig, msg);\n};\n\nmpl.figure.prototype.close_ws = function (fig, msg) {\n    fig.send_message('closing', msg);\n    // fig.ws.close()\n};\n\nmpl.figure.prototype.push_to_output = function (_remove_interactive) {\n    // Turn the data on the canvas into data in the output cell.\n    var width = this.canvas.width / this.ratio;\n    var dataURL = this.canvas.toDataURL();\n    this.cell_info[1]['text/html'] =\n        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n};\n\nmpl.figure.prototype.updated_canvas_event = function () {\n    // Tell IPython that the notebook contents must change.\n    IPython.notebook.set_dirty(true);\n    this.send_message('ack', {});\n    var fig = this;\n    // Wait a second, then push the new image to the DOM so\n    // that it is saved nicely (might be nice to debounce this).\n    setTimeout(function () {\n        fig.push_to_output();\n    }, 1000);\n};\n\nmpl.figure.prototype._init_toolbar = function () {\n    var fig = this;\n\n    var toolbar = document.createElement('div');\n    toolbar.classList = 'btn-toolbar';\n    this.root.appendChild(toolbar);\n\n    function on_click_closure(name) {\n        return function (_event) {\n            return fig.toolbar_button_onclick(name);\n        };\n    }\n\n    function on_mouseover_closure(tooltip) {\n        return function (event) {\n            if (!event.currentTarget.disabled) {\n                return fig.toolbar_button_onmouseover(tooltip);\n            }\n        };\n    }\n\n    fig.buttons = {};\n    var buttonGroup = document.createElement('div');\n    buttonGroup.classList = 'btn-group';\n    var button;\n    for (var toolbar_ind in mpl.toolbar_items) {\n        var name = mpl.toolbar_items[toolbar_ind][0];\n        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n        var image = mpl.toolbar_items[toolbar_ind][2];\n        var method_name = mpl.toolbar_items[toolbar_ind][3];\n\n        if (!name) {\n            /* Instead of a spacer, we start a new button group. */\n            if (buttonGroup.hasChildNodes()) {\n                toolbar.appendChild(buttonGroup);\n            }\n            buttonGroup = document.createElement('div');\n            buttonGroup.classList = 'btn-group';\n            continue;\n        }\n\n        button = fig.buttons[name] = document.createElement('button');\n        button.classList = 'btn btn-default';\n        button.href = '#';\n        button.title = name;\n        button.innerHTML = '<i class=\"fa ' + image + ' fa-lg\"></i>';\n        button.addEventListener('click', on_click_closure(method_name));\n        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n        buttonGroup.appendChild(button);\n    }\n\n    if (buttonGroup.hasChildNodes()) {\n        toolbar.appendChild(buttonGroup);\n    }\n\n    // Add the status bar.\n    var status_bar = document.createElement('span');\n    status_bar.classList = 'mpl-message pull-right';\n    toolbar.appendChild(status_bar);\n    this.message = status_bar;\n\n    // Add the close button to the window.\n    var buttongrp = document.createElement('div');\n    buttongrp.classList = 'btn-group inline pull-right';\n    button = document.createElement('button');\n    button.classList = 'btn btn-mini btn-primary';\n    button.href = '#';\n    button.title = 'Stop Interaction';\n    button.innerHTML = '<i class=\"fa fa-power-off icon-remove icon-large\"></i>';\n    button.addEventListener('click', function (_evt) {\n        fig.handle_close(fig, {});\n    });\n    button.addEventListener(\n        'mouseover',\n        on_mouseover_closure('Stop Interaction')\n    );\n    buttongrp.appendChild(button);\n    var titlebar = this.root.querySelector('.ui-dialog-titlebar');\n    titlebar.insertBefore(buttongrp, titlebar.firstChild);\n};\n\nmpl.figure.prototype._remove_fig_handler = function (event) {\n    var fig = event.data.fig;\n    if (event.target !== this) {\n        // Ignore bubbled events from children.\n        return;\n    }\n    fig.close_ws(fig, {});\n};\n\nmpl.figure.prototype._root_extra_style = function (el) {\n    el.style.boxSizing = 'content-box'; // override notebook setting of border-box.\n};\n\nmpl.figure.prototype._canvas_extra_style = function (el) {\n    // this is important to make the div 'focusable\n    el.setAttribute('tabindex', 0);\n    // reach out to IPython and tell the keyboard manager to turn it's self\n    // off when our div gets focus\n\n    // location in version 3\n    if (IPython.notebook.keyboard_manager) {\n        IPython.notebook.keyboard_manager.register_events(el);\n    } else {\n        // location in version 2\n        IPython.keyboard_manager.register_events(el);\n    }\n};\n\nmpl.figure.prototype._key_event_extra = function (event, _name) {\n    // Check for shift+enter\n    if (event.shiftKey && event.which === 13) {\n        this.canvas_div.blur();\n        // select the cell after this one\n        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n        IPython.notebook.select(index + 1);\n    }\n};\n\nmpl.figure.prototype.handle_save = function (fig, _msg) {\n    fig.ondownload(fig, null);\n};\n\nmpl.find_output_cell = function (html_output) {\n    // Return the cell and output element which can be found *uniquely* in the notebook.\n    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n    // IPython event is triggered only after the cells have been serialised, which for\n    // our purposes (turning an active figure into a static one), is too late.\n    var cells = IPython.notebook.get_cells();\n    var ncells = cells.length;\n    for (var i = 0; i < ncells; i++) {\n        var cell = cells[i];\n        if (cell.cell_type === 'code') {\n            for (var j = 0; j < cell.output_area.outputs.length; j++) {\n                var data = cell.output_area.outputs[j];\n                if (data.data) {\n                    // IPython >= 3 moved mimebundle to data attribute of output\n                    data = data.data;\n                }\n                if (data['text/html'] === html_output) {\n                    return [cell, data, j];\n                }\n            }\n        }\n    }\n};\n\n// Register the function which deals with the matplotlib target/channel.\n// The kernel may be null if the page has been refreshed.\nif (IPython.notebook.kernel !== null) {\n    IPython.notebook.kernel.comm_manager.register_target(\n        'matplotlib',\n        mpl.mpl_figure_comm\n    );\n}\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id='4411790c-f1d7-4d2b-98aa-54754d5508bf'></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\javie\\git\\uma_robotics_2023\\robotica\\lib\\site-packages\\numpy\\core\\shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ary = asanyarray(ary)\n",
      "c:\\Users\\javie\\git\\uma_robotics_2023\\robotica\\lib\\site-packages\\matplotlib\\cbook\\__init__.py:1369: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return np.asarray(x, float)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clear_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [13], line 21\u001b[0m\n\u001b[0;32m     17\u001b[0m xVehicleTrue \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack([\u001b[39m-\u001b[39mMapSize\u001b[39m/\u001b[39m\u001b[39m1.5\u001b[39m, \u001b[39m-\u001b[39mMapSize\u001b[39m/\u001b[39m\u001b[39m1.5\u001b[39m, \u001b[39m0.\u001b[39m]) \u001b[39m# We know the exact robot pose at any moment\u001b[39;00m\n\u001b[0;32m     19\u001b[0m robot \u001b[39m=\u001b[39m EFKMappingRobot(xVehicleTrue, Sigma_r, Sigma_theta, nLandmarks)\n\u001b[1;32m---> 21\u001b[0m demo_ekf_mapping(robot, Map ,nLandmarks, mode\u001b[39m=\u001b[39;49mmode)\n",
      "Cell \u001b[1;32mIn [12], line 64\u001b[0m, in \u001b[0;36mdemo_ekf_mapping\u001b[1;34m(robot, Map, nLandmarks, mode, logger, nSteps, turning, print_each)\u001b[0m\n\u001b[0;32m     61\u001b[0m             plt\u001b[39m.\u001b[39maxis([\u001b[39m-\u001b[39mMapSize\u001b[39m-\u001b[39m\u001b[39m5\u001b[39m, MapSize\u001b[39m+\u001b[39m\u001b[39m5\u001b[39m, \u001b[39m-\u001b[39mMapSize\u001b[39m-\u001b[39m\u001b[39m5\u001b[39m, MapSize\u001b[39m+\u001b[39m\u001b[39m5\u001b[39m]) \u001b[39m# Set limits again\u001b[39;00m\n\u001b[0;32m     62\u001b[0m             \u001b[39m#plt.draw()\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39m#           plt.savefig(str(k)+'.jpg')\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m             clear_output(wait\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     65\u001b[0m             display(canvas\u001b[39m.\u001b[39mfig) \n\u001b[0;32m     67\u001b[0m             \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstep_by_step\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clear_output' is not defined"
     ]
    }
   ],
   "source": [
    "#mode = 'step_by_step'\n",
    "mode = 'visualize_process'\n",
    "#mode = 'non_stop'\n",
    "\n",
    "# WORLD MAP\n",
    "# Num features/landmarks considered within the map\n",
    "nLandmarks = 1\n",
    "# Generation of the map\n",
    "MapSize = 100\n",
    "Map = MapSize*random.rand(2,nLandmarks)-MapSize/2\n",
    "\n",
    "# ROBOT\n",
    "# Covariances for our very bad&expensive sensor (in the system <d,theta>)\n",
    "Sigma_r = 8.0\n",
    "Sigma_theta = 8*np.pi/180\n",
    "# Initial robot pose\n",
    "xVehicleTrue = np.vstack([-MapSize/1.5, -MapSize/1.5, 0.]) # We know the exact robot pose at any moment\n",
    "\n",
    "robot = EFKMappingRobot(xVehicleTrue, Sigma_r, Sigma_theta, nLandmarks)\n",
    "\n",
    "demo_ekf_mapping(robot, Map ,nLandmarks, mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considering a larger number of landmarks \n",
    "\n",
    "Once our EKF implementation is working with one landmark, let's try it in a scenario with 5 landmarks. Again, the content of the `xEst` and `Pest` is shown after each 5 iterations of the algorithm.\n",
    "    \n",
    "<figure style=\"text-align:center\">\n",
    "  <img src=\"images/fig6-1-3.png\" width=\"500\" alt=\"\">\n",
    "  <figcaption>\n",
    "      Fig. 2: Execution of the EKF algorithmn for mapping (multiple landmarks). <br/>\n",
    "      Same as in Fig 1., each landmark is accompanied by a number of times observed.\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mode = 'step_by_step'\n",
    "mode = 'visualize_process'\n",
    "#mode = 'non_stop'\n",
    "\n",
    "# WORLD MAP\n",
    "# Num features/landmarks considered within the map\n",
    "nLandmarks = 5\n",
    "# Generation of the map\n",
    "MapSize = 100\n",
    "Map = MapSize*random.rand(2,nLandmarks)-MapSize/2\n",
    "\n",
    "# ROBOT\n",
    "# Covariances for our very bad&expensive sensor (in the system <d,theta>)\n",
    "Sigma_r = 8.0\n",
    "Sigma_theta = 7*np.pi/180\n",
    "# Initial robot pose\n",
    "xVehicleTrue = np.vstack([-MapSize/1.5, -MapSize/1.5, 0.]) # We know the exact robot pose at any moment\n",
    "\n",
    "robot = EFKMappingRobot(xVehicleTrue, Sigma_r, Sigma_theta, nLandmarks)\n",
    "\n",
    "demo_ekf_mapping(robot, Map ,nLandmarks, mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"><b><i>Thinking about it (1)</i></b></font>\n",
    "\n",
    "Having completed these trials, you will be able to **answer the following questions**:\n",
    "\n",
    "In the **one landmark** case:\n",
    "\n",
    "- Discuss the evolution of the variables `xEst` and `Pest`, including their dimensions.\n",
    "\n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "    \n",
    "In the **five landmarks** case:\n",
    "\n",
    "- Why and how the content of the variables `xEst` and `Pest` has change? Discuss their size.\n",
    "\n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "\n",
    "- What structure does the matrix of covariances have? Is there any kind of correlation among the observations of different landmarks?\n",
    "\n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting performance results\n",
    "\n",
    "As is normal, the contracting company requires some information about how well our EFK implementation performs. For that, your colleagues have implemented a logger, which is meant to store some information each loop regarding the method performance and plot it at the end of its execution. **Execute the following code cells and take a look at that plots!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class Logger():\n",
    "    \"\"\" Logs info about the covariance and error of a map.\n",
    "    \n",
    "        Attrs:\n",
    "            n_features: Number of features in the world.\n",
    "            log_error: Matrix to store the error in the fitting for each landmark.\n",
    "            log_det: Matrix to store the determinant of the covariance matrix for each landmark.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, n_steps, n_features):\n",
    "        \"\"\" Initializes each matrix to log the information\n",
    "        \n",
    "            Args:\n",
    "                n_steps: Maximum number of steps our robot will take.\n",
    "                n_features: Number of features in the world.\n",
    "        \"\"\"\n",
    "        self.n_features = n_features\n",
    "        self.log_error = np.empty((n_steps,n_features))\n",
    "        self.log_det = np.empty((n_steps,n_features))\n",
    "            \n",
    "    def log(self, k: int, robot: EFKMappingRobot, Map: np.ndarray):\n",
    "        \"\"\" Computes relevant info about the error and covariances.\n",
    "        \n",
    "            It is called once per loop in the demo.\n",
    "        \n",
    "            Args:\n",
    "                k: Number of iteration we are at. Range: [0, n_steps)\n",
    "                robot: \n",
    "                Map:\n",
    "        \"\"\"\n",
    "        for idx in range(self.n_features):\n",
    "            tid = robot.MappedLandmarks[idx,0]\n",
    "            if tid <= -1:\n",
    "                self.log_error[k,idx]= np.Inf\n",
    "                self.log_det[k,idx]=np.Inf\n",
    "            else:\n",
    "                self.log_det[k,idx] = np.linalg.det(robot.PEst[tid:tid+2,tid:tid+2])\n",
    "                self.log_error[k,idx] = np.sqrt(np.sum((robot.xEst[tid:tid+2,0] - Map[:,idx])**2))\n",
    "                \n",
    "    def plot(self):\n",
    "        \"\"\" Plot all relevant figures. It is called at the end of the demo\"\"\"\n",
    "        fig1 , ax1 =plt.subplots(1, 1, sharex=True)\n",
    "        fig2 , ax2 =plt.subplots(1, 1, sharex=True)\n",
    "        fig2.tight_layout()\n",
    "        fig1.tight_layout()\n",
    "\n",
    "        df1 = pd.DataFrame(data= self.log_error, columns = ['Landmark {}'.format(i) for i in range(self.n_features)])\n",
    "        ax1.set_title('Error between map and est')\n",
    "        df1.plot(ax = ax1)\n",
    "        df2 = pd.DataFrame(data=np.log(self.log_det), columns=['Landmark {}'.format(i) for i in range(self.n_features)])\n",
    "        ax2.set_title('Det. of covar.')\n",
    "        df2.plot(ax = ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mode = 'step_by_step'\n",
    "mode = 'visualize_process'\n",
    "#mode = 'non_stop'\n",
    "\n",
    "# WORLD MAP\n",
    "# Num features/landmarks considered within the map\n",
    "nLandmarks = 5\n",
    "# Generation of the map\n",
    "MapSize = 100\n",
    "Map = MapSize*random.rand(2,nLandmarks)-MapSize/2\n",
    "\n",
    "# ROBOT\n",
    "# Covariances for our very bad&expensive sensor (in the system <d,theta>)\n",
    "Sigma_r = 8.0\n",
    "Sigma_theta = 7*np.pi/180\n",
    "# Initial robot pose\n",
    "xVehicleTrue = np.vstack([-MapSize/1.5, -MapSize/1.5, 0.]) # We know the exact robot pose at any moment\n",
    "\n",
    "robot = EFKMappingRobot(xVehicleTrue, Sigma_r, Sigma_theta, nLandmarks)\n",
    "\n",
    "nSteps=100\n",
    "logger = Logger(n_features=nLandmarks, n_steps=nSteps)\n",
    "\n",
    "demo_ekf_mapping(robot,\n",
    "                 Map,\n",
    "                 nLandmarks,\n",
    "                 logger=logger,\n",
    "                 mode='non_stop',\n",
    "                 nSteps=nSteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"><b><i>Thinking about it (2)</i></b></font>\n",
    "\n",
    "Having taken a look at the logger and its output, you will be able to **answer the following questions**:\n",
    "\n",
    "- What information is shown in the figures produced by the logger?\n",
    "\n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "    \n",
    "- The information about the error and the determinant of the covariance is provided for first time at different iterations of the algorithm for each landmark. Is that an error? Why is this happening?\n",
    "\n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "\n",
    "- The error associated to each landmark not always decreases with new observations. Why could this happen?\n",
    "\n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "    \n",
    "- On the contrary, the determinant of the covariance matrix associated to each landmark always decreases when new observations are available. Is this an error? Why?\n",
    "\n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('robotica': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "28e9071c3a84e675c861ccac27ca2f83f0b4f889f18eee2378f13f1cc33790dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
